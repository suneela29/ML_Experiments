# 🚀 ML Classification Experiments  

This repository contains two **machine learning classification experiments** using well-known datasets.  
The goal is to explore different **supervised learning algorithms** and evaluate their performance on diverse classification tasks.  

## 🔍 Digits Classification (`digits_classification.ipynb`)  
- **Dataset:** Handwritten Digits Dataset (from sklearn)  
- **Goal:** Classify handwritten digits (0-9)  
- **Algorithms Used:**  
  - ✅ K-Nearest Neighbors (KNN)  
  - ✅ Gaussian Naive Bayes (GNB)  
- **Performance Metric:** F1 Score (Macro)  
- **Visualization:** Displays test digits and their nearest neighbors  

---

## 🔬 Breast Cancer Classification (`cancer_classification.ipynb`)  
- **Dataset:** Breast Cancer Wisconsin Dataset (from sklearn)  
- **Goal:** Classify breast cancer as **malignant** or **benign**  
- **Algorithms Used:**  
  - ✅ AdaBoost  
  - ✅ Support Vector Machine (SVM)  
  - ✅ Multi-layer Perceptron (MLP)  
- **Performance Metric:** Accuracy Score  

---

## ⚡ How to Run the Notebooks  
1. **Clone this repository:**  
   ```bash
   git clone https://github.com/yourusername/ML-Experiments.git
   cd ML-Experiments
## 🎯 Key Takeaways  

### 📌 Digits Classification  
- KNN is more accurate but slower compared to Naive Bayes.  

### 📌 Cancer Classification  
- SVM and MLP outperform AdaBoost in this dataset.  

---

## 💡 Future Improvements  
- 🔹 Hyperparameter tuning for better model performance  
- 🔹 Implementing deep learning models (CNNs for digits)  
- 🔹 Experimenting with feature engineering  

---

## 🤝 Contributing  
Want to improve these models? Feel free to fork, experiment, and submit a pull request! 🚀  
