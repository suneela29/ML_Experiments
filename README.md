# ğŸš€ ML Classification Experiments  

This repository contains two **machine learning classification experiments** using well-known datasets.  
The goal is to explore different **supervised learning algorithms** and evaluate their performance on diverse classification tasks.  

## ğŸ” Digits Classification (`digits_classification.ipynb`)  
- **Dataset:** Handwritten Digits Dataset (from sklearn)  
- **Goal:** Classify handwritten digits (0-9)  
- **Algorithms Used:**  
  - âœ… K-Nearest Neighbors (KNN)  
  - âœ… Gaussian Naive Bayes (GNB)  
- **Performance Metric:** F1 Score (Macro)  
- **Visualization:** Displays test digits and their nearest neighbors  

---

## ğŸ”¬ Breast Cancer Classification (`cancer_classification.ipynb`)  
- **Dataset:** Breast Cancer Wisconsin Dataset (from sklearn)  
- **Goal:** Classify breast cancer as **malignant** or **benign**  
- **Algorithms Used:**  
  - âœ… AdaBoost  
  - âœ… Support Vector Machine (SVM)  
  - âœ… Multi-layer Perceptron (MLP)  
- **Performance Metric:** Accuracy Score  

---

## âš¡ How to Run the Notebooks  
1. **Clone this repository:**  
   ```bash
   git clone https://github.com/yourusername/ML-Experiments.git
   cd ML-Experiments
## ğŸ¯ Key Takeaways  

### ğŸ“Œ Digits Classification  
- KNN is more accurate but slower compared to Naive Bayes.  

### ğŸ“Œ Cancer Classification  
- SVM and MLP outperform AdaBoost in this dataset.  

---

## ğŸ’¡ Future Improvements  
- ğŸ”¹ Hyperparameter tuning for better model performance  
- ğŸ”¹ Implementing deep learning models (CNNs for digits)  
- ğŸ”¹ Experimenting with feature engineering  

---

## ğŸ¤ Contributing  
Want to improve these models? Feel free to fork, experiment, and submit a pull request! ğŸš€  
